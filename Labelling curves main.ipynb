{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef326da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import packages necessary to run script\n",
    "'''\n",
    "# def import_packages():\n",
    "# import tkinter as tk\n",
    "# from tkinter import filedialog, simpledialog, messagebox\n",
    "import math\n",
    "from statistics import mean, stdev\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "# import os\n",
    "# %matplotlib inline\n",
    "# from datetime import datetime\n",
    "# import matplotlib.pyplot as plt\n",
    "# import scipy.stats as sps\n",
    "# import sys\n",
    "# import scipy.optimize as spo\n",
    "# return print('packages imported')\n",
    "# import_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fetch working directory, and set modules directory\n",
    "'''\n",
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "modules_path = cwd + '\\Modules\\\\'\n",
    "sys.path.insert(1, modules_path)\n",
    "print(f\"Added {modules_path} as modules folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Set up input and output folders and scan settings\n",
    "'''\n",
    "def initial_setup(addfolderswithdate, getinput_type):\n",
    "    import InitialSetup as InitialSetup\n",
    "    input_output_folders = InitialSetup.get_folders(withdate=addfolderswithdate)\n",
    "    settings = InitialSetup.get_data_type(get_input=True)\n",
    "    return (input_output_folders, settings)\n",
    "\n",
    "addfolderswithdate, getinput_type = False, True #change this to make new folder with date\n",
    "initial_setup_output = initial_setup(addfolderswithdate, getinput_type)\n",
    "file_path_input = initial_setup_output[0][0]\n",
    "file_path_output = initial_setup_output[0][1]\n",
    "spectra_scan_type = initial_setup_output[1][0]\n",
    "areas_scan_type = initial_setup_output[1][1]\n",
    "spectra_check_string = initial_setup_output[1][2]\n",
    "print(f\"Copy input files to /{file_path_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for retrieving csv file and returning in indicated format\n",
    "'''\n",
    "def get_csv(return_type, title, initialdir, skip_blank_lines, skiprows):\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    import GeneralFunctions as GeneralFunctions\n",
    "    csvfile = tk.filedialog.askopenfilename(title=title, initialdir=initialdir, filetype=((\"CSV files\", \".csv\"), (\"all files\", \"*.*\")))\n",
    "    file = open(csvfile)\n",
    "    if return_type == 'lines':\n",
    "        file_lines = GeneralFunctions.get_file_lines(file)\n",
    "        file.close()\n",
    "        return(file_lines)\n",
    "    elif return_type == 'df' :\n",
    "        file_df = pd.read_csv(file, skip_blank_lines=skip_blank_lines, skiprows=skiprows)\n",
    "        file.close()\n",
    "        return(file_df)\n",
    "    elif return_type == 'both':\n",
    "        file_df = pd.read_csv(file, skip_blank_lines=skip_blank_lines, skiprows=skiprows)\n",
    "        file.close()\n",
    "        file = open(csvfile)\n",
    "        file_lines = GeneralFunctions.get_file_lines(file)\n",
    "        file.close()\n",
    "        return(file_lines, file_df)\n",
    "print(\"defined function for retrieving csv files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d30f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fetch profile in both formats\n",
    "'''\n",
    "# profile_lines, profile_df = get_csv(return_type='both', title='Profile', initialdir=file_path_input)\n",
    "profile_df = get_csv(return_type='df', title='Profile', initialdir=file_path_input, skip_blank_lines=False, skiprows=None)\n",
    "# can change the return type to lines once I add legacy functionality back in\n",
    "print(\"profile file imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16042c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Count the number of features in the profile dataframe\n",
    "'''\n",
    "max_feature_count = max(profile_df.count())\n",
    "min_feature_count = min(profile_df.count())\n",
    "print(f\"{max_feature_count} total features\\n {max_feature_count-min_feature_count} feature(s) missing profile data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c63868",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Format and Display the first 5 lines of the profile dataframe\n",
    "'''\n",
    "# profile_df['RT'] = profile_df['RT'].astype(float)\n",
    "# profile_df['Start'] = profile_df['Start'].astype(float)\n",
    "# profile_df['End'] = profile_df['End'].astype(float)\n",
    "# profile_df['#Carbons'] = profile_df['#Carbons'].astype(int)\n",
    "# profile_df['Quasi Mol ion'] = profile_df['Quasi Mol ion'].astype(float)\n",
    "# profile_df['MW'] = profile_df['MW'].astype(float)\n",
    "# profile_df['Quasi Mol ion'] = profile_df['Quasi Mol ion'].apply(np.floor)\n",
    "# profile_df['MW'] = profile_df['MW'].apply(np.floor)\n",
    "def rename_duplicates(input_series):\n",
    "    renamed_series = input_series\n",
    "    series_list = []\n",
    "    for item in input_series:\n",
    "        counter = 1\n",
    "        new_item = str(item)\n",
    "        while new_item in series_list:\n",
    "            counter += 1\n",
    "            new_item = str(item) + '_' + str(counter)\n",
    "        series_list.append(new_item)\n",
    "    renamed_series_values = pd.Series(series_list)\n",
    "    return renamed_series_values\n",
    "\n",
    "profile_df['Peak'] = rename_duplicates(profile_df['Peak'])\n",
    "profile_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53329a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df['Peak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd978489",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fetch sample spectra as filelines\n",
    "'''\n",
    "spectra_lines = get_csv(return_type='lines', title='Spectra', initialdir=file_path_input, skip_blank_lines=False, skiprows=None)\n",
    "print(\"spectra file imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de63b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Count the total number of spectra in the spectra file\n",
    "'''\n",
    "count_spectra = len(spectra_lines)\n",
    "print(f\"{count_spectra} total spectra lines imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a897d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fetch the spectrum for each feature in the spectra input file\n",
    "'''\n",
    "\n",
    "def get_sampleID(line_string):\n",
    "    ## works to return sample ID from full info line\n",
    "    ## #\"+CI Scan (rt: 4.762 min) 210505_NH3-pos-split-scan_361-R2.D  Subtract \"\n",
    "    start_character = \"min) \"\n",
    "    end_character = \".D\"\n",
    "    start_index = line_string.rfind(start_character) + len(start_character)\n",
    "    end_index = line_string.rfind(end_character)\n",
    "    sampleID = line_string[start_index:end_index]\n",
    "    return sampleID\n",
    "\n",
    "def get_spectrum_rt(sample_details):\n",
    "    ## works to return rt of spectrum from info line\n",
    "    ## #\"+CI Scan (rt: 4.762 min) 210505_NH3-pos-split-scan_361-R2.D  Subtract \"\n",
    "    left_character = \"(rt: \"\n",
    "    right_character = \" min)\"\n",
    "    left_index = sample_details.find(left_character) + len(left_character)\n",
    "    right_index = sample_details.find(right_character)\n",
    "    rt = float(sample_details[left_index:right_index])\n",
    "    return rt\n",
    "\n",
    "def fetch_spectra_from_lines(spectra_lines, spectra_check_string):\n",
    "    print(spectra_check_string)\n",
    "    all_features_list = []\n",
    "    ### Add in all features to a list with [[sample name, [rt,[xs],[ys]],[rt,[xs],[ys]],...],[sample name, [rt,[xs],[ys]],[rt,[xs],[ys]],...],...]\n",
    "    sample_name = \"no sample name imported\"\n",
    "    first_spectrum = True\n",
    "    feature_rt = int\n",
    "    exceptions = 0\n",
    "    \n",
    "    line_counter = 0\n",
    "    feature_counter = 0\n",
    "    for line in spectra_lines:\n",
    "        line_counter += 1\n",
    "        # go through each line in the spectra file\n",
    "        if \"#\" in line:\n",
    "        # info line or headers\n",
    "            if spectra_check_string in line:\n",
    "                # sample & spectra information line\n",
    "                # new spectrum\n",
    "                feature_counter +=1\n",
    "                \n",
    "                if first_spectrum:\n",
    "                    #do not append previous spectrum if this is the first one (otherwise appends empty spectrum)\n",
    "                    spectrum_x, spectrum_y, this_spectrum, this_feature = [],[],[],[] # prepare initial empty lists\n",
    "                    first_spectrum = False # add next spectrum to list of all spectra\n",
    "                elif not first_spectrum:\n",
    "                    # add spectrum to list of all spectra\n",
    "                    #this_spectrum.append(spectrum_x)\n",
    "                    #this_spectrum.append(spectrum_y)\n",
    "                    #this_feature.append(this_spectrum)\n",
    "                    this_feature.append(spectrum_x)\n",
    "                    this_feature.append(spectrum_y)\n",
    "                    all_features_list.append(this_feature)\n",
    "                spectrum_x, spectrum_y, this_spectrum, this_feature = [],[],[],[] # reset to prepare for next feature with the following properties\n",
    "                # set the following parameters after appending the previous feature information\n",
    "                spectrum_rt = get_spectrum_rt(line)\n",
    "                sample_ID = get_sampleID(line)\n",
    "                this_feature = [sample_ID, spectrum_rt]\n",
    "    #             print(all_features_list)\n",
    "\n",
    "                \n",
    "        elif len(line) > 2:\n",
    "            #line containing single m/z and abundance [point,m/z,abundance]\n",
    "            this_line = line.strip()\n",
    "            this_line = this_line.split(',')\n",
    "            \n",
    "            try:\n",
    "                spectrum_x.append(float(this_line[1]))\n",
    "            except:\n",
    "                print(f'\"error in \" {this_line}')\n",
    "            spectrum_y.append(float(this_line[2]))\n",
    "                \n",
    "    \n",
    "    # final spectrum must be appended to the list\n",
    "#     this_spectrum.append(spectrum_x)\n",
    "#     this_spectrum.append(spectrum_y)\n",
    "    this_feature.append(spectrum_x)\n",
    "    this_feature.append(spectrum_y)\n",
    "#     this_feature.append(this_spectrum)\n",
    "    all_features_list.append(this_feature)\n",
    "#     print(all_features_list)\n",
    "    print(f\"{line_counter} lines processed; spectra extracted for {feature_counter} total features\")\n",
    "    return all_features_list # output as [[sample name,rt,[xs],[ys]],[sample name,rt,[xs],[ys]],...]\n",
    "print(\"defined functions for extracting spectra from csv file\")\n",
    "\n",
    "\n",
    "unsorted_spectra = fetch_spectra_from_lines(spectra_lines, spectra_check_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007389dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sort and organize the list of spectra as a dataframe\n",
    "'''\n",
    "def sort_spectra_list(unsorted_spectra):\n",
    "    columnheaders = 'SampleID', 'RT', 'm/z list', 'Abundance list' # headers for the dataframe\n",
    "    unsorted_spectra_df = pd.DataFrame(unsorted_spectra, columns = columnheaders) # make a dataframe from the unsorted spectra list with indicated headers\n",
    "    sorted_spectra_df = unsorted_spectra_df # copy the dataframe before sorting\n",
    "    sorted_spectra_df = sorted_spectra_df.sort_values(['SampleID', 'RT']) # sort by SampleID then by RT of spectrum\n",
    "    return sorted_spectra_df # retrun the sorted spectra as a dataframe\n",
    "print(\"defined the function for sorting spectra\")\n",
    "\n",
    "sorted_spectra_df = sort_spectra_list(unsorted_spectra)\n",
    "\n",
    "print(f\"sorted all imported spectra\\n{len(sorted_spectra_df)} spectra retained from {sorted_spectra_df['SampleID'].nunique()} distinct samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf99b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_spectra_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_spectra_df = sorted_spectra_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Still need to comment this section\n",
    "'''\n",
    "def roundedmz(mz):\n",
    "    mzfloor = math.floor(mz)\n",
    "    decimal = mz - mzfloor\n",
    "    if decimal > 0.7:\n",
    "        return math.ceil(mz)\n",
    "    else:\n",
    "        return mzfloor\n",
    "\n",
    "def calculate_rawF(xs, ys, Quasi_mol_ion, ncarbons):\n",
    "    upperx = Quasi_mol_ion + ncarbons + 0.7\n",
    "    lowerx = Quasi_mol_ion - 0.3\n",
    "    rawF_df = pd.DataFrame({'x':xs,'y':ys})\n",
    "    rawF_df['m/z for label calc'] = rawF_df['y'].where((rawF_df['x'] >= lowerx) & (rawF_df['x'] <= upperx), 0)\n",
    "    abund_sum =  rawF_df['m/z for label calc'].sum()\n",
    "    rawF_df['RelAbund'] = rawF_df['m/z for label calc'] / abund_sum\n",
    "    rawF_df['roundx'] = rawF_df['x'].apply(roundedmz)\n",
    "    rawF_df['labels'] = (rawF_df['x'] - lowerx).where((rawF_df['m/z for label calc'] > 0), None)\n",
    "    rawF_df['F'] = rawF_df['RelAbund'] * rawF_df['labels'] / ncarbons\n",
    "    rawF = rawF_df['F'].sum()\n",
    "    return rawF, rawF_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This section is for getting all the spectra into a table and include most likely profile match. the other one is for matching spectra to the profile.\n",
    "Still need to comment this section\n",
    "'''\n",
    "def match_profile_to_spectra(detailed_spectra_df, profile_df, minabund):\n",
    "    #RT error if RT outside of profile bounds, Abund error if Max(Abund) < x\n",
    "    details_list = []\n",
    "    # details_list = [in_profile_column,likely_peak_column,likely_rt_column,rt_diff,likely_QuasiMolIon_column,likely_ncarbons_column,rawF_column]\n",
    "    columnheaders = ['Error Flags','In profile?','ProfileID','ProfileRT','RT difference','Profile quasi mol ion', 'Profile #carbons', 'RawF']\n",
    "    minabund = minabund # minabundance below this value records an error flag in the output\n",
    "    details_all_features = []\n",
    "    print(\"Aligning all features with possible profile matches\")\n",
    "    for index,row in detailed_spectra_df.iterrows():\n",
    "        spectrum = rt,xs,ys = row[1],row[2],row[3]\n",
    "        possible_features = []\n",
    "        in_profile = False\n",
    "        for index1,profilefeature in profile_df.iterrows():\n",
    "            this_feature = []\n",
    "            flag = None\n",
    "            lowerrt, upperrt = profilefeature['Start'],profilefeature['End']\n",
    "            if lowerrt < rt < upperrt: # feature rt is within profile rt bounds\n",
    "                # feature confirmed in profile\n",
    "                flag = None\n",
    "                maxy = float(max(ys))\n",
    "                if maxy < minabund:\n",
    "                    flag = 'Abund error'\n",
    "                in_profile = True\n",
    "                profileID, profileRT, profile_quasimolion, profile_ncarbons = profilefeature['Peak'], profilefeature['RT'], profilefeature['Quasi Mol ion'], profilefeature['#Carbons']\n",
    "                RT_difference = rt - profilefeature['RT']\n",
    "                RT_difference = round(RT_difference,4)\n",
    "                rawF, rawF_df = calculate_rawF(xs, ys, Quasi_mol_ion=profilefeature['Quasi Mol ion'], ncarbons=profilefeature['#Carbons'])\n",
    "                this_feature=[flag,in_profile,profileID,profileRT,RT_difference,profile_quasimolion,profile_ncarbons,rawF]\n",
    "                possible_features.append(this_feature)\n",
    "        if in_profile:\n",
    "            possible_features_df = pd.DataFrame(possible_features,columns=columnheaders)\n",
    "            this_feature_df = possible_features_df[possible_features_df['RT difference'] == possible_features_df['RT difference'].min()]\n",
    "            '''\n",
    "            I want to add in functionality here to compare abundance of quasi molecular ion and weight selection by abundance and RT difference\n",
    "            '''\n",
    "            this_feature = this_feature_df.loc[0].tolist()\n",
    "        else:\n",
    "            flag = 'RT error' # feature not found in profile\n",
    "            profileID, profileRT, RT_difference, profile_quasimolion, profile_ncarbons, rawF = None, None, None, None, None, None\n",
    "            this_feature=[flag,in_profile,profileID,profileRT,RT_difference,profile_quasimolion,profile_ncarbons,rawF]\n",
    "        details_all_features.append(this_feature)\n",
    "\n",
    "    details_df = pd.DataFrame(details_all_features, columns=columnheaders, index=detailed_spectra_df.index)\n",
    "    all_features_with_details_df = pd.concat([detailed_spectra_df, details_df], axis=1, ignore_index=False, sort=False)\n",
    "    filtered_features_with_details_df = all_features_with_details_df[all_features_with_details_df['In profile?'] == True]\n",
    "    return all_features_with_details_df, filtered_features_with_details_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82741986",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This section is for getting the most likely spectrum for each profile feature.\n",
    "Still need to comment this section\n",
    "'''\n",
    "def match_spectra_to_profile(detailed_spectra_df, profile_df, minabund):\n",
    "    #RT error if RT outside of profile bounds, Abund error if Max(Abund) < x\n",
    "    minabund = minabund # maxabundance below this value records an error flag in the output\n",
    "    detailed_spectra_df_split = [y for x, y in detailed_spectra_df.groupby('SampleID', as_index=False)] # Split dataframe by SampleID\n",
    "    profile_headers = profile_df.columns.tolist()\n",
    "    extra_headers = ['SampleID','In profile?','Error Flags','RT difference','RT','m/z list','abundance list','RawF']\n",
    "    columnheaders = profile_headers + extra_headers\n",
    "    all_rows = [columnheaders]\n",
    "    \n",
    "    unique_samples = pd.unique(detailed_spectra_df['SampleID']).tolist()\n",
    "    simplified_headers = profile_headers + unique_samples\n",
    "    all_simplified_features = []\n",
    "    \n",
    "    super_simple_headers = ['Peak'] + unique_samples\n",
    "    all_super_simple_features = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index1,profilefeature in profile_df.iterrows():\n",
    "        profileID, profileRT, profile_quasimolion, profile_ncarbons = profilefeature['Peak'], profilefeature['RT'], profilefeature['Quasi Mol ion'], profilefeature['#Carbons']\n",
    "        profile_line = profilefeature.tolist()\n",
    "        this_simplified_feature = profilefeature.tolist()\n",
    "        this_super_simple_feature = [profileID]\n",
    "        this_feature = []\n",
    "        flag = None\n",
    "        lowerrt, upperrt = profilefeature['Start'],profilefeature['End']\n",
    "        previous_sample = None\n",
    "        first_run_for_feature = True\n",
    "                \n",
    "        for sample in detailed_spectra_df_split:\n",
    "            in_profile=False\n",
    "            possible_features = []\n",
    "            sampleID = sample['SampleID'].iloc[0]\n",
    "#             print(sampleID)\n",
    "            \n",
    "            for index1,feature in sample.iterrows():\n",
    "                this_feature = []\n",
    "                spectrum = rt,xs,ys = feature[1],feature[2],feature[3]\n",
    "                if lowerrt < rt < upperrt: # feature rt is within profile rt bounds\n",
    "                    # feature confirmed in profile\n",
    "                    flag = None\n",
    "                    in_profile = True\n",
    "                    try:\n",
    "                        maxy = float(max(ys))\n",
    "                    except:\n",
    "                        maxy = 0\n",
    "                        \n",
    "                    if maxy < minabund:\n",
    "                        flag = 'Abund error'\n",
    "                    \n",
    "                    RT_difference = rt - profileRT\n",
    "                    RT_difference = round(RT_difference,4)\n",
    "                    this_feature=[flag,RT_difference,rt,xs,ys]\n",
    "                    possible_features.append(this_feature)\n",
    "            if in_profile:\n",
    "                possible_features_df = pd.DataFrame(possible_features,columns=['flag','RT_difference','rt','xs','ys'])\n",
    "                \n",
    "                this_feature_df = possible_features_df[possible_features_df['RT_difference'] == possible_features_df['RT_difference'].min()]\n",
    "                this_feature_df = this_feature_df.loc[0]\n",
    "                \n",
    "                xs, ys = this_feature_df['xs'], this_feature_df['ys']\n",
    "                rawF, rawF_df = calculate_rawF(xs, ys, Quasi_mol_ion=profile_quasimolion, ncarbons=profile_ncarbons)\n",
    "                \n",
    "                this_feature = [sampleID, in_profile]\n",
    "                this_feature = this_feature + (this_feature_df.tolist())\n",
    "                this_feature.append(rawF)\n",
    "\n",
    "                this_row = profile_line + this_feature\n",
    "                all_rows.append(this_row)\n",
    "                this_simplified_feature.append(rawF)\n",
    "                this_super_simple_feature.append(rawF)\n",
    "                \n",
    "#                 print(f\"{profileID} added for {sampleID}\")\n",
    "            else:\n",
    "                this_simplified_feature.append(None)\n",
    "                this_super_simple_feature.append(None)\n",
    "#                 print(f\"{profileID} not found for {sampleID}\")\n",
    "        all_simplified_features.append(this_simplified_feature)\n",
    "        all_super_simple_features.append(this_super_simple_feature)\n",
    "#         print(f\"{profileID} completed\")\n",
    "    \n",
    "    detailed_rawF_df = pd.DataFrame(all_rows,columns=columnheaders)\n",
    "    simplified_rawF_df = pd.DataFrame(all_simplified_features,columns=simplified_headers)\n",
    "    super_simple_rawF_df = pd.DataFrame(all_super_simple_features, columns = super_simple_headers)\n",
    "    return(detailed_rawF_df, simplified_rawF_df, super_simple_rawF_df)\n",
    "\n",
    "#     return(detailed_rawF_df, simplificed_rawF_df)\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7e523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prioritization_type = 'profile' # 'profile' or 'sample'\n",
    "print(f\"{data_prioritization_type} data prioritization type selected\")\n",
    "\n",
    "\n",
    "\n",
    "if data_prioritization_type == 'sample':\n",
    "    all_features_with_details_df, filtered_features_with_details_df = match_profile_to_spectra(detailed_spectra_df, profile_df, minabund=10)\n",
    "    print(\"Dataframe for all features with details complete\")\n",
    "    output_file_name = 'all_features_with_details_RawF.csv'\n",
    "    print(f\"Exporting all_features_with_details_df as {output_file_name} to {file_path_output}\")\n",
    "    output_file_name_path = os.path.join(file_path_output, output_file_name)\n",
    "    try:\n",
    "        all_features_with_details_df.to_csv(output_file_name_path, sep=',', na_rep='', index=False)\n",
    "        print(\"Export complete\")\n",
    "    except:\n",
    "        print(f\"Error in exporting {output_file_name} to {output_file_name_path}.\\nMake sure a file with this name is not already open.\\nContinuing without exporting this file.\")\n",
    "\n",
    "    all_features_with_details_df.head()\n",
    "else:\n",
    "    print(\"Processing detailed rawF data table...\")\n",
    "    detailed_rawF_df, simplified_rawF_df, super_simple_rawF_df = match_spectra_to_profile(detailed_spectra_df, profile_df, minabund=10)\n",
    "    print(\"Dataframe for all features with details complete\")\n",
    "    output_file_name1 = 'detailed_rawF_df.csv'\n",
    "    output_file_name2 = 'simplified_rawF_df.csv'\n",
    "    print(f\"Exporting {output_file_name1} and {output_file_name2} to {file_path_output}\")\n",
    "    output_file_name_path1 = os.path.join(file_path_output, output_file_name1)\n",
    "    output_file_name_path2 = os.path.join(file_path_output, output_file_name2)\n",
    "    try:\n",
    "        detailed_rawF_df.to_csv(output_file_name_path1, sep=',', na_rep='', index=False)\n",
    "        print(\"Export complete\")\n",
    "    except:\n",
    "        print(f\"Error in exporting {output_file_name1} to {output_file_name_path1}.\\nMake sure a file with this name is not already open.\\nContinuing without exporting this file.\")\n",
    "    try:\n",
    "        simplified_rawF_df.to_csv(output_file_name_path2, sep=',', na_rep='', index=False)\n",
    "        print(\"Export complete\")\n",
    "    except:\n",
    "        print(f\"Error in exporting {output_file_name2} to {output_file_name_path2}.\\nMake sure a file with this name is not already open.\\nContinuing without exporting this file.\")\n",
    "\n",
    "    simplified_rawF_df.head()\n",
    "print(\"all exports complete\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_simple_rawF_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4419dbcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Section incomplete and temporarily abandoned in favor of profile prioritization\n",
    "'''\n",
    "\n",
    "# if data_prioritization_type == 'sample':\n",
    "#     filtered_features_with_details_df['Profile quasi mol ion'] = filtered_features_with_details_df['Profile quasi mol ion'].astype(int)\n",
    "#     filtered_features_with_details_df['Profile #carbons'] = filtered_features_with_details_df['Profile #carbons'].astype(int)\n",
    "#     filtered_features_with_details_df.head()\n",
    "#     short_df = filtered_features_with_details_df[['SampleID','ProfileID','RawF']]\n",
    "#     short_df.head()\n",
    "#     '''\n",
    "#     So far this method seems far less efficient than my old one that iterated through lists\n",
    "#     '''\n",
    "#     unique_samples = pd.unique(short_df['SampleID'])\n",
    "#     print(f\"Fetching simplified labeling data for {len(unique_samples)} unique sample(s): {unique_samples}\")\n",
    "#     combined_df = profile_df.drop(columns = ['Area','Height','Width','FWHM','Name','RawMW'])\n",
    "#     for sample in unique_samples:\n",
    "#         print(f\"working on {sample}\")\n",
    "#         this_df = short_df[short_df['SampleID'] == sample]\n",
    "#         this_df = this_df.drop(columns = ['SampleID'])\n",
    "#         this_df.rename(columns={'ProfileID':'Peak', 'RawF':sample}, inplace=True)\n",
    "#         combined_df = pd.merge(combined_df, this_df, how='left',on=[\"Peak\"])\n",
    "#     combined_df.head()\n",
    "#     print(f\"combined RawF data from all {len(unique_samples)} unique sample(s) into a single dataframe\")\n",
    "#     #simple_rawF_df\n",
    "#     simple_rawF_list = [] # All samples will have the RawF assigned to a single list for each feature to make one table of RawF values\n",
    "#     headers = ['SampleID'] # The first column will be Sample ID\n",
    "#     profile_features_list = []\n",
    "#     for index1,profilefeature in profile_df.iterrows(): # Iterate through each feature in the profile\n",
    "#         peak = profilefeature['Peak'] # Get the name of this feature from the dataframe\n",
    "#         headers.append(peak) # Each feature (peak) in the profile has its name added to the list of headers\n",
    "#         profile_features_list.append(peak)\n",
    "\n",
    "#     print(profile_features_list)\n",
    "\n",
    "#     first_run = True\n",
    "#     for index1,sample in filtered_features_with_details_df.iterrows(): # iterate through each row in the dataframe\n",
    "#         sampleID = sample['SampleID'] # get sampleID from dataframe for this feature\n",
    "#         if first_run: # Check if this is the first row in the dataframe\n",
    "#             previous_sampleID = sampleID\n",
    "#             this_feature = [sampleID]\n",
    "#             first_run = False # Don't use this function anymore after it has been run once\n",
    "\n",
    "#         if sampleID == previous_sampleID: # same sample\n",
    "#             print('same sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_areas_df(file_path_input):\n",
    "    print(\"fetching areas\")\n",
    "    areas_df = get_csv(return_type='df', title='Areas', initialdir=file_path_input, skip_blank_lines=True, skiprows=1)\n",
    "    areas_df.columns.values[0] = 'SampleID'\n",
    "    areas_df.head()\n",
    "    return areas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampleID_from_area(sample_file):\n",
    "    ## works to return sample ID from full info line\n",
    "    ## #\"+CI Scan (rt: 4.762 min) 210505_NH3-pos-split-scan_361-R2.D  Subtract \"\n",
    "    start_character = \"\\\\\"\n",
    "    end_character = \".D\"\n",
    "    start_index = sample_file.rfind(start_character) + len(start_character)\n",
    "    end_index = sample_file.rfind(end_character)\n",
    "    sampleID = sample_file[start_index:end_index]\n",
    "    return sampleID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c33f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area_tables(profile_df, areas_df):\n",
    "    print(\"Separating features by sample\")\n",
    "    areas_df_split = [y for x, y in areas_df.groupby('SampleID', as_index=False)] # Split dataframe by SampleID\n",
    "\n",
    "    profile_headers = profile_df.columns.tolist()\n",
    "    extra_headers = ['sampleID', 'in_profile', 'flag','RT_difference']\n",
    "    area_headers = areas_df.columns.tolist()\n",
    "    feature_headers = extra_headers + area_headers\n",
    "    columnheaders = profile_headers + extra_headers + area_headers\n",
    "    all_rows = [columnheaders]\n",
    "\n",
    "    simplified_headers = profile_headers\n",
    "    all_simplified_features = []\n",
    "\n",
    "    first_run_for_sample = True\n",
    "    for index1,profilefeature in profile_df.iterrows():\n",
    "        profileID, profileRT, lowerrt, upperrt = profilefeature['Peak'], profilefeature['RT'], profilefeature['Start'], profilefeature['End']\n",
    "#         print(f\"Working on: {profileID}\")\n",
    "        profile_line = profilefeature.tolist()\n",
    "        this_simplified_feature = profilefeature.tolist()\n",
    "        this_feature = []\n",
    "        flag, previous_sample, first_run_for_feature = None, None, True\n",
    "        sample_counter = 0 \n",
    "        for sample_df in areas_df_split:\n",
    "            sample_df['RT'] = pd.to_numeric(sample_df['RT'], errors='coerce') # Sets non numeric as NaN\n",
    "            sample_df = sample_df[sample_df['RT'] > 0]\n",
    "            if len(sample_df) > 0: # Only process data files and lines with at least 1 features\n",
    "                sample_file = sample_df['SampleID'].iloc[0]\n",
    "                sampleID = get_sampleID_from_area(sample_file)                 \n",
    "#                 print(f\"processing: {sampleID}\")\n",
    "                if first_run_for_sample:\n",
    "                    simplified_headers.append(sampleID)\n",
    "                in_profile=False\n",
    "                possible_features = []\n",
    "\n",
    "    #                 rt_index = sample_df.columns.get_loc(\"RT\")\n",
    "    #                 area_index = sample_df.columns.get_loc(\"Area\")\n",
    "                for index1,feature in sample_df.iterrows():\n",
    "                    this_feature = []\n",
    "                    rt,area = feature['RT'],feature['Area']\n",
    "                    if lowerrt < rt < upperrt: # feature rt is within profile rt bounds\n",
    "                        # feature confirmed in profile\n",
    "                        flag = None\n",
    "                        in_profile = True\n",
    "                        RT_difference = rt - profileRT\n",
    "                        RT_difference = round(RT_difference,4)\n",
    "                        this_feature=[sampleID, in_profile, flag, RT_difference]\n",
    "                        feature_values = feature.values.tolist()\n",
    "                        this_feature = this_feature + feature_values\n",
    "                        possible_features.append(this_feature)\n",
    "\n",
    "                if in_profile:\n",
    "                    sample_counter += 1\n",
    "                    possible_features_df = pd.DataFrame(possible_features,columns=feature_headers)\n",
    "                    this_feature_df = possible_features_df[possible_features_df['RT_difference'] == possible_features_df['RT_difference'].min()]\n",
    "                    this_feature_df = this_feature_df.loc[0]\n",
    "                    area = this_feature_df['Area']\n",
    "                    this_feature = this_feature_df.tolist()\n",
    "                    this_row = profile_line + this_feature\n",
    "                    all_rows.append(this_row)\n",
    "                    this_simplified_feature.append(area)\n",
    "    #                 print(f\"{profileID} added for {sampleID}\")\n",
    "                else:\n",
    "                    this_simplified_feature.append(None)\n",
    "    #                 print(f\"{profileID} not found for {sampleID}\")\n",
    "        all_simplified_features.append(this_simplified_feature)\n",
    "        first_run_for_sample = False\n",
    "        print(f\"{profileID} found in {sample_counter} samples.\")\n",
    "            \n",
    "    #         print(f\"{profileID} completed\")\n",
    "#     simplified_area_table = simplified_headers + all_simplified_features\n",
    "    detailed_areas_df = pd.DataFrame(all_rows,columns=columnheaders)\n",
    "    simplified_areas_df = pd.DataFrame(all_simplified_features,columns=simplified_headers)\n",
    "    return(detailed_areas_df, simplified_areas_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_areas():\n",
    "    '''ask if user wants to get area data'''\n",
    "    from tkinter.messagebox import askyesno\n",
    "    title = 'Process peak area data?'\n",
    "    message = 'Would you like to process peak area data?'\n",
    "    get_areas = askyesno(title=title, message=message)\n",
    "    return get_areas\n",
    "\n",
    "def process_areas(file_path_input, file_path_output):\n",
    "    get_areas = ask_areas()\n",
    "    if get_areas is True:\n",
    "        areas_df = get_areas_df(file_path_input)\n",
    "        print(\"Processing areas data table...\")\n",
    "        detailed_areas_df, simplified_areas_df = get_area_tables(profile_df, areas_df)\n",
    "        print(\"Dataframe for all features with details complete\")\n",
    "        output_file_name1 = 'detailed_areas_df.csv'\n",
    "        output_file_name2 = 'simplified_areas_df.csv'\n",
    "        print(f\"Exporting {output_file_name1} and {output_file_name2} to {file_path_output}\")\n",
    "        output_file_name_path1 = os.path.join(file_path_output, output_file_name1)\n",
    "        output_file_name_path2 = os.path.join(file_path_output, output_file_name2)\n",
    "        try:\n",
    "            detailed_areas_df.to_csv(output_file_name_path1, sep=',', na_rep='', index=False, header=False)\n",
    "            print(\"Export complete\")\n",
    "        except:\n",
    "            print(f\"Error in exporting {output_file_name1} to {output_file_name_path1}.\\nMake sure a file with this name is not already open.\\nContinuing without exporting this file.\")\n",
    "        try:\n",
    "            simplified_areas_df.to_csv(output_file_name_path2, sep=',', na_rep='', index=False, header=True)\n",
    "            print(\"Export complete\")\n",
    "        except:\n",
    "            print(f\"Error in exporting {output_file_name2} to {output_file_name_path2}.\\nMake sure a file with this name is not already open.\\nContinuing without exporting this file.\")\n",
    "\n",
    "        simplified_areas_df.head()\n",
    "    else:\n",
    "        print(\"No peak area analysis\")\n",
    "        # quit()\n",
    "        simplified_areas_df = pd.DataFrame()\n",
    "    return simplified_areas_df\n",
    "        \n",
    "simplified_areas_df= process_areas(file_path_input, file_path_output)\n",
    "use_areas = False\n",
    "if simplified_areas_df.empty != True:\n",
    "    use_areas = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (use_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f661a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "functional above here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Need do double check below here\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get sample info\n",
    "'''\n",
    "def get_info_df(file_path_input):\n",
    "    print(\"fetching sample info\")\n",
    "    info_df = get_csv(return_type='df', title='Info', initialdir=file_path_input, skip_blank_lines=True, skiprows=0)\n",
    "    return info_df\n",
    "\n",
    "info_df = get_info_df(file_path_input)\n",
    "print(\"sample info imported\")\n",
    "info_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd91aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Format RawF data for use with sample info\n",
    "'''\n",
    "# super_simple_rawF_df_transposed = super_simple_rawF_df.set_index('Peak')\n",
    "super_simple_rawF_df_transposed = super_simple_rawF_df.rename(columns={'Peak':'Sample index'})\n",
    "super_simple_rawF_df_transposed = super_simple_rawF_df_transposed.set_index('Sample index')\n",
    "super_simple_rawF_df_transposed = super_simple_rawF_df_transposed.transpose()\n",
    "super_simple_rawF_df_transposed['SampleID'] = super_simple_rawF_df_transposed.index\n",
    "\n",
    "\n",
    "\n",
    "super_simple_rawF_df_transposed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_name_format = True #If using custom sample ID indicators then set True\n",
    "# custom_sample_ID_start = '230726' #Enter in the desired sample number start indicator (only used if new_sample_name_format is True)\n",
    "# custom_sample_ID_end = '_sim' #Enter in the desired sample number end indicator (only used if new_sample_name_format is True)\n",
    "custom_sample_ID_start = '220302_NH3_PCI-fs_' #Enter in the desired sample number start indicator (only used if new_sample_name_format is True)\n",
    "custom_sample_ID_end = '.D' #Enter in the desired sample number end indicator (only used if new_sample_name_format is True)\n",
    "\n",
    "if new_sample_name_format:\n",
    "    sample_ID_start = custom_sample_ID_start #Enter in the desired sample number start indicator\n",
    "    sample_ID_start_length = len(sample_ID_start)\n",
    "    sample_ID_end = custom_sample_ID_end #Enter in the desired sample number end indicator\n",
    "    print(f\"Using new sample name format from \\'{sample_ID_start}\\' to \\'{sample_ID_end}\\'.\")\n",
    "else:\n",
    "    print(\"Using default sample name format\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_number(sampleID):\n",
    "    '''\n",
    "    Adding in functionality for new sample naming conventions\n",
    "    Old:\n",
    "    start = sampleID.rfind('_') + 1\n",
    "    sample_number = sampleID[start:]\n",
    "    return sample_number\n",
    "    '''\n",
    "    print(sampleID)\n",
    "    if new_sample_name_format is True:\n",
    "        start = sampleID.find(sample_ID_start) + sample_ID_start_length\n",
    "        # print(f'start: {start}')\n",
    "        end = sampleID.rfind(sample_ID_end) + len(sampleID) + 1\n",
    "        # print(f'end: {end}')\n",
    "        sample_number = sampleID[start:end]\n",
    "    else:\n",
    "        start = sampleID.rfind('_') + 1\n",
    "        sample_number = sampleID[start:]\n",
    "    print(sample_number)\n",
    "    return sample_number\n",
    "def get_sample_number_column(transposed_df1):\n",
    "    transposed_df2 = transposed_df1\n",
    "    sample_number_list = []\n",
    "    for sampleID in transposed_df2['SampleID']:\n",
    "        try:\n",
    "            sample_number = get_sample_number(sampleID)\n",
    "        except:\n",
    "            sample_number = np.nan\n",
    "        sample_number_list.append(sample_number)\n",
    "    transposed_df2['Sample number'] = sample_number_list\n",
    "    print(\"Sample number column added\")\n",
    "    return transposed_df2\n",
    "simplified_rawF_df_transposed = get_sample_number_column(super_simple_rawF_df_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_rawF_df_transposed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_areas == True:\n",
    "    simplified_areas_df_transposed = simplified_areas_df.rename(columns={'Peak':'Sample index'})\n",
    "    simplified_areas_df_transposed = simplified_areas_df_transposed.set_index('Sample index')\n",
    "    simplified_areas_df_transposed = simplified_areas_df_transposed.transpose()\n",
    "    simplified_areas_df_transposed['SampleID'] = simplified_areas_df_transposed.index\n",
    "# simplified_areas_df_transposed\n",
    "    simplified_areas_df_transposed = get_sample_number_column(simplified_areas_df_transposed)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merge sample info and RawF (this table could be exported)\n",
    "'''\n",
    "def merge_dataframes(left_df, right_df, column):\n",
    "    # print(f'left: {left_df}')\n",
    "    left = left_df\n",
    "    left[column]=left[column].astype(str)\n",
    "    # print(f'right: {right_df}')\n",
    "    right = right_df\n",
    "    right[column]=right[column].astype(str)\n",
    "\n",
    "\n",
    "    merged_df = pd.merge(left, right, on=column)\n",
    "    merged_df.columns = merged_df.columns.str.replace(\"\\r\", \"\")\n",
    "    merged_df.columns = merged_df.columns.str.replace(\"\\n\", \"\")\n",
    "    merged_df.columns = merged_df.columns.str.replace(\"predicted\", \"\")\n",
    "    return merged_df\n",
    "df_AlignedRawFTableAndInfo = merge_dataframes(left_df=simplified_rawF_df_transposed, right_df=info_df, column='Sample number')\n",
    "print('dataframes merged')\n",
    "df_AlignedRawFTableAndInfo.index = df_AlignedRawFTableAndInfo['SampleID']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99814169",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Old\n",
    "def get_internal_standard(peak_list):\n",
    "    print(f'Peak List: {peak_list}')\n",
    "    internal_standard = '*Tropate (2TMS)'\n",
    "    print(f'Internal Standard: {internal_standard}')\n",
    "    return internal_standard\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' in progress 230804 - MB\n",
    "import ipywidgets as widgets\n",
    "def get_internal_standard(feature_options):\n",
    "    @widgets.interact(internalstnd=widgets.Dropdown(options=feature_options, description='Select IS:', disabled=confirmation))\n",
    "    def select_internalstnd(internalstnd):\n",
    "        # print(internalstnd)\n",
    "        return internalstnd\n",
    "def get_confirmation():\n",
    "    @widgets.interact(confirmation_button=widgets.ToggleButton(value=False, description='Click to confirm selection', disabled=False, button_style='', tooltip='Description',icon='check'))\n",
    "    def select_confirmation(confirmation_button):\n",
    "        while confirmation_button is True:\n",
    "            waiting=True\n",
    "            print(\"Done waiting\")\n",
    "            return(internal_standard)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' in progress 230804 - MB\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "run_this = True\n",
    "\n",
    "\n",
    "if run_this:\n",
    "    # @widgets.interact(internalstnd=widgets.Dropdown(options=feature_options, description='Select IS:', disabled=confirmation))\n",
    "    # def select_internalstnd(internalstnd):\n",
    "    #     # print(internalstnd)\n",
    "    #     return internalstnd\n",
    "    internal_standard = get_internal_standard(feature_options = ['ribitol','shikimate','pyruvate'])\n",
    "    confirmation = get_confirmation()\n",
    "    print(confirmation)\n",
    "    print(internal_standard)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b46719",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' in progress 230804 - MB\n",
    "if use_areas == True:\n",
    "    not_confirmed = True\n",
    "    peak_list=profile_df['Peak'].tolist()\n",
    "    internal_standard = get_internal_standard(peak_list)\n",
    "\n",
    "\n",
    "    print(f\"{internal_standard} selected as the internal standard\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e59461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_IS():\n",
    "    internal_standard = '*Tropate (2TMS)'\n",
    "    return internal_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75370e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_IS = True\n",
    "\n",
    "if custom_IS == True:\n",
    "    # print(df_AlignedRawFTableAndInfo.columns)\n",
    "    internal_standard = ask_for_IS()\n",
    "else:\n",
    "    internal_standard = 'ribitol'\n",
    "\n",
    "if use_areas == True:\n",
    "    \n",
    "    df_AlignedRawAreaTableAndInfo = merge_dataframes(left_df=simplified_areas_df_transposed, right_df=info_df, column='Sample number')\n",
    "    df_AlignedRawAreaTableAndInfo.index = df_AlignedRawAreaTableAndInfo['SampleID']\n",
    "    df_RelAreasAndInfo = df_AlignedRawAreaTableAndInfo\n",
    "#     InternalStandard = '*Tropate (2TMS)'\n",
    "    peak_list=profile_df['Peak'].tolist()\n",
    "    # internal_standard = get_internal_standard(peak_list)\n",
    "    # internal_standard = 'ribitol'\n",
    "    for feature in profile_df['Peak']:\n",
    "        df_RelAreasAndInfo[feature] = df_RelAreasAndInfo[feature].astype(float)/df_AlignedRawAreaTableAndInfo[internal_standard].astype(float)\n",
    "#     print(df_RelAreasAndInfo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Separate into labeled and unlabeled\n",
    "'''\n",
    "def check_unlabelled(df): # add in a column (series) to dataframe for if sample is unlabeled\n",
    "    new_df = df.sort_values('Total labelling time (min) corrected') # sort the dataframe based on labelling time (this will stack the unlabeled samples at the top)\n",
    "    new_df['Unlabelled'] = np.where(new_df['Total labelling time (min) corrected'] == 0, True, False) # new column in dataframe called 'Unlabelled' is True if labeling time is 0 \n",
    "    return new_df # return the dataframe containing a new column for if the samples are labeled or unlabeled\n",
    "\n",
    "def oulier_removed_series(series): # calculate quartiles and remove outliers from series\n",
    "    Q1 = series.quantile(0.25) # quartile 1\n",
    "    Q3 = series.quantile(0.75) # quartile 3\n",
    "    IQR = Q3 - Q1 # interquartile range\n",
    "    new_series = series.where((series > (Q1 - 1.5 * IQR)) & (series < (Q3 + 1.5 * IQR)), np.nan) # replace outliers with NaN\n",
    "    return new_series # return series with outliers replaced by NaNs\n",
    "\n",
    "def get_unlabelled_df(df,start_index,end_index): # get a dataframe containing only the unlabeled samples\n",
    "    df_AlignedRawFTableAndInfo = df # already added 'Unlabelled' column\n",
    "    unlabelled_df = df_AlignedRawFTableAndInfo[df_AlignedRawFTableAndInfo['Unlabelled'] == True] # keep rows only where 'unlabelled' column is True\n",
    "    for feature in unlabelled_df.iloc[:,start_index:end_index]: # go through each feature in unlabelled samples df\n",
    "        unlabelled_df[feature] = oulier_removed_series(unlabelled_df[feature]) # remove outliers from unlabelled samples for each feature to provide a more reliable unlabelled/background calculation\n",
    "    return unlabelled_df # return the dataframe of unlabeled samples with outliers removed\n",
    "\n",
    "def calc_F_all(df_AlignedRawFTableAndInfo):\n",
    "    \n",
    "    df_AlignedRawFTableAndInfo = check_unlabelled(df_AlignedRawFTableAndInfo) # Creates new column ['Unlabelled'] where True implies sample is unlabelled control\n",
    "    index_column_name = 'SampleID' # indicate the name of the dataframe column to use for the index\n",
    "    index_column = df_AlignedRawFTableAndInfo[index_column_name] # index the dataframe based on the previously defined column\n",
    "    \n",
    "    last_column_index = df_AlignedRawFTableAndInfo.columns.get_loc(index_column_name) # Get the index for the plant_number column (where the data stops and the sample info begins) \n",
    "    \n",
    "    unlabelled_df = get_unlabelled_df(df_AlignedRawFTableAndInfo, start_index=0, end_index=last_column_index)\n",
    "    cols = list(df_AlignedRawFTableAndInfo.columns)\n",
    "    df_AlignedRawFTableAndInfo.loc[df_AlignedRawFTableAndInfo[index_column_name].isin(unlabelled_df[index_column_name]), cols] = unlabelled_df[cols]    \n",
    "    \n",
    "    average_unlabelled_df = df_AlignedRawFTableAndInfo.groupby('Unlabelled').mean() # Group by samples that are unlabelled (True) or unlabelled (False) and retreive the mean.\n",
    "    average_unlabelled_df.insert(0, \"Unlabelled\", [\"False\",\"True\"])\n",
    "    average_unlabelled_df.insert(0, \"Variable\", [\"Mean\",\"Mean\"])\n",
    "    SD_unlabelled_df = df_AlignedRawFTableAndInfo.groupby('Unlabelled').std()\n",
    "    SD_unlabelled_df.insert(0, \"Unlabelled\", [\"False\",\"True\"])\n",
    "    SD_unlabelled_df.insert(0, \"Variable\", [\"SD\",\"SD\"])\n",
    "    N_unlabelled_df = df_AlignedRawFTableAndInfo.groupby('Unlabelled').count()\n",
    "    N_unlabelled_df.insert(0, \"Unlabelled\", [\"False\",\"True\"])\n",
    "    N_unlabelled_df.insert(0, \"Variable\", [\"N\",\"N\"])\n",
    "    SE_unlabelled_df = df_AlignedRawFTableAndInfo.groupby('Unlabelled').sem()\n",
    "    SE_unlabelled_df.insert(0, \"Unlabelled\", [\"False\",\"True\"])\n",
    "    SE_unlabelled_df.insert(0, \"Variable\", [\"SE\",\"SE\"])\n",
    "    df_F_raw_unlabelled = pd.concat([average_unlabelled_df, SD_unlabelled_df, N_unlabelled_df, SE_unlabelled_df])\n",
    "    \n",
    "    df_F_normalized = pd.DataFrame(index = index_column) # Makes a new dataframe call df_F_normalized with the index of df1 (sample IDs)\n",
    "    df_F_normalizedto100 = pd.DataFrame(index = index_column) # use for curves, not for amount of labelled carbon\n",
    "    df_F_normalized['Sample'] = index_column # The first column in the table is also added to the dataframe under the header 'Sample'\n",
    "    df_F_normalizedto100['Sample'] = index_column \n",
    "    \n",
    "    for feature in df_AlignedRawFTableAndInfo.iloc[:,:last_column_index]: # Iterate through each feature excluding column 0 which has 'Sample' and the columns including and following 'plant_number'\n",
    "        '''\n",
    "        Get average_unlabelled after removing outliers because this throws off all F normalizations\n",
    "        '''\n",
    "        average_unlabelled = average_unlabelled_df[feature] # Get the average_unlabelled column for this feature from the average_unlabelled_df\n",
    "        \n",
    "        average_unlabelled = average_unlabelled[1] # Get the mean of unlabelled samples for this feature. index [1] is for the unlabelled samples\n",
    "        df_AlignedRawFTableAndInfo[feature] = pd.to_numeric(df_AlignedRawFTableAndInfo[feature], downcast = 'float') # Make data numeric\n",
    "#         df_F_normalized[feature] = np.where(df_AlignedRawFTableAndInfo[feature] >= min_unlabelled, df_AlignedRawFTableAndInfo[feature] - average_unlabelled, np.nan) # If the sample labelling is greater than or equal to unlabelled (natural abundance) then subtract average_unlabelled, else make datapoint Nan\n",
    "        \n",
    "#     df_F_normalized[feature] = np.where((df_AlignedRawFTableAndInfo[feature] >= average_unlabelled) & (df_AlignedRawFTableAndInfo['Unlabelled'] == False), df_AlignedRawFTableAndInfo[feature] - average_unlabelled, np.where(df_AlignedRawFTableAndInfo['Unlabelled'] == True, df_AlignedRawFTableAndInfo[feature], np.nan)) # If the sample labelling is greater than or equal to unlabelled (natural abundance) then subtract average_unlabelled, else make datapoint Nan\n",
    "        \n",
    "    \n",
    "        correction_to_100percent = 1 - average_unlabelled\n",
    "#         df_F_normalized[feature] = np.where((df_AlignedRawFTableAndInfo['Unlabelled'] == True), (df_AlignedRawFTableAndInfo[feature] - average_unlabelled), (np.where((df_AlignedRawFTableAndInfo[feature] >= average_unlabelled), (df_AlignedRawFTableAndInfo[feature] - average_unlabelled), np.nan)))\n",
    "        df_F_normalized[feature] = np.where((df_AlignedRawFTableAndInfo['Unlabelled'] == True), (df_AlignedRawFTableAndInfo[feature] - average_unlabelled), (np.where((df_AlignedRawFTableAndInfo[feature] >= average_unlabelled), (df_AlignedRawFTableAndInfo[feature] - average_unlabelled), 0)))\n",
    "\n",
    "        df_F_normalizedto100[feature] = df_F_normalized[feature] / correction_to_100percent\n",
    "        \n",
    "        #         df_F_normalized[feature] = np.where((df_AlignedRawFTableAndInfo[feature] == True), (df_AlignedRawFTableAndInfo[feature] - (average_unlabelled), df_AlignedRawFTableAndInfo[feature]))\n",
    "#         df_F_normalized[feature] = np.where((df_AlignedRawFTableAndInfo[feature] == False & df_AlignedRawFTableAndInfo[]), (df_AlignedRawFTableAndInfo[feature] - (average_unlabelled), df_AlignedRawFTableAndInfo[feature]))\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    for column in df_AlignedRawFTableAndInfo.iloc[:,last_column_index:]:\n",
    "         df_F_normalized[column] = df_AlignedRawFTableAndInfo[column]\n",
    "    df_F_normalizedto100['Group'] = df_F_normalized['Group']\t\n",
    "    df_F_normalizedto100['Total labelling time (min) corrected'] = df_F_normalized['Total labelling time (min) corrected']\n",
    "    return [df_F_normalized, df_F_raw_unlabelled, df_F_normalizedto100]\n",
    "\n",
    "\n",
    "\n",
    "df_F_normalized, df_F_raw_unlabelled, df_F_normalizedto100 = calc_F_all(df_AlignedRawFTableAndInfo)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_F_normalized, df_F_raw_unlabelled, df_F_normalizedto100\n",
    "df_F_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0425cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_F_normalizedto100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file_name = 'NormalizedF.csv'\n",
    "# output_file_name_path = os.path.join(file_path_output, output_file_name)\n",
    "# df_F_normalized.to_csv(output_file_name_path, sep=',', na_rep='', index=False)\n",
    "\n",
    "# output_file_name = 'Unlabelled_averages.csv'\n",
    "# output_file_name_path = os.path.join(file_path_output, output_file_name)\n",
    "# df_F_raw_unlabelled.to_csv(output_file_name_path, sep=',', na_rep='', index=False)\n",
    "\n",
    "# output_file_name = 'NormalizedF_to100percent.csv'\n",
    "# output_file_name_path = os.path.join(file_path_output, output_file_name)\n",
    "# df_F_normalizedto100.to_csv(output_file_name_path, sep=',', na_rep='', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099de4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99240e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_normalized_F_csvs(list_of_dfs, list_of_file_names, file_path_output):\n",
    "    index = 0\n",
    "    print(\"Exporting dataframes to csv\")\n",
    "    for df in list_of_dfs:\n",
    "        try:\n",
    "            output_file_name = list_of_file_names[index]\n",
    "            output_file_name_path = os.path.join(file_path_output, output_file_name)\n",
    "            df.to_csv(output_file_name_path, sep=',', na_rep='', index=False)\n",
    "            print(f\"Exported {output_file_name}\")\n",
    "        except:\n",
    "            print(f\"Failed to export {output_file_name} to {file_path_output}. A file with the same name may already be open.\")\n",
    "        index += 1\n",
    "    return print(\"Done\")\n",
    "\n",
    "list_of_dfs = [df_F_normalized, df_F_raw_unlabelled, df_F_normalizedto100]\n",
    "list_of_file_names =['NormalizedF.csv', 'Unlabelled_averages.csv', 'NormalizedF_to100percent.csv'] \n",
    "export_normalized_F_csvs(list_of_dfs, list_of_file_names, file_path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_of_dfs = {}\n",
    "for idx, df in enumerate(list_of_dfs):\n",
    "    filename = list_of_file_names[idx]\n",
    "    df_name = filename.replace('.csv','_df')\n",
    "    dictionary_of_dfs[df_name] = df\n",
    "    if use_areas == True:\n",
    "        dictionary_of_dfs['raw_areas_df'] = df_AlignedRawFTableAndInfo\n",
    "        dictionary_of_dfs['rel_areas_df'] = df_RelAreasAndInfo\n",
    "    \n",
    "    \n",
    "for name,df in dictionary_of_dfs.items():\n",
    "    print(f'{name} added to dictionary_of_dfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767bc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Separate dataframes into groups\n",
    "'''\n",
    "treatment_dfs_dictionary = {}\n",
    "treatment_dfs_dictionary_RawAreas = {}\n",
    "treatment_dfs_dictionary_RelAreas = {}\n",
    "working_df_Fto100 = dictionary_of_dfs['NormalizedF_to100percent_df']\n",
    "if use_areas == True:\n",
    "    working_df_RawAreas = dictionary_of_dfs['raw_areas_df']\n",
    "    working_df_RelAreas = dictionary_of_dfs['rel_areas_df']\n",
    "# print(working_df_Fto100['Group'])\n",
    "treatments = working_df_Fto100['Group'].unique()\n",
    "number_of_treatments = len(treatments)\n",
    "print(f'{treatments} treatments')\n",
    "for treatment in treatments:\n",
    "    df = working_df_Fto100[working_df_Fto100['Group'] == treatment]\n",
    "    treatment_dfs_dictionary[treatment] = df\n",
    "    if use_areas == True:\n",
    "        df2 = working_df_RawAreas[working_df_RawAreas['Group'] == treatment]\n",
    "        treatment_dfs_dictionary_RawAreas[treatment] = df2\n",
    "        df3 = working_df_RelAreas[working_df_RelAreas['Group'] == treatment]\n",
    "        treatment_dfs_dictionary_RelAreas[treatment] = df3\n",
    "#     print(df.Group)\n",
    "    \n",
    "for name,df in treatment_dfs_dictionary.items():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c85233",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot data on graph\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b531df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ttest_ind_from_stats\n",
    "from scipy.special import stdtr\n",
    "import scipy.optimize as spo\n",
    "import scipy.stats as sps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e418a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t, A, k):\n",
    "    solution = A * (1-np.exp(-k * t))\n",
    "    return solution\n",
    "\n",
    "def curve_fitting(dataframe_x_y, random_iterations, color, plot_in_console):\n",
    "    '''\n",
    "    Dataframe should by 2 columns ('x' & 'y') with '''\n",
    "    for i in range(random_iterations):\n",
    "\n",
    "        A = random.random()\n",
    "        k = random.random()\n",
    "        x0 = [A,k]\n",
    "\n",
    "        this_data = dataframe_x_y[pd.isna(dataframe_x_y.y) == False]\n",
    "        this_data.sort_values(by=['x'])\n",
    "\n",
    "        x = this_data['x'].values\n",
    "        y = this_data['y'].values\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                result = spo.curve_fit(model, x, y, bounds = ((0,0),(1,np.inf)), method = 'trf') # with bounds, but won't use lm by default\n",
    "        #         result = spo.curve_fit(model, x, y, method='lm')\n",
    "            except:\n",
    "                try:\n",
    "                    A = random.random()\n",
    "                    k = random.random()\n",
    "                    x0 = [A,k]\n",
    "                    result = spo.curve_fit(model, x, y, bounds = ((0,0),(1,np.inf)), method = 'trf') # with bounds, but won't use lm by default\n",
    "                except:\n",
    "                    try:\n",
    "                        result = spo.curve_fit(model, x, y,  bounds = ((0,0),(1,np.inf)), method = 'dogbox')\n",
    "                    except:\n",
    "                        try:\n",
    "                            result = spo.curve_fit(model, x, y, method = 'lm')\n",
    "                        except:\n",
    "                            try:\n",
    "                                result = spo.curve_fit(model, x, y, method = 'trf')\n",
    "                            except:\n",
    "                                try:\n",
    "                                    result = spo.curve_fit(model, x, y, method = 'dogbox')\n",
    "                                except:\n",
    "                                    try:\n",
    "                                        result = spo.minimize(get_RSDR, x0, args=this_data, bounds = ((0,0),(1,np.inf)))\n",
    "                                    except:\n",
    "                                        print(\"Could not generate a curve\")\n",
    "            A, k = coeffs = result[0]\n",
    "            pcov = result[1]\n",
    "#             To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))\n",
    "            A_SD, k_SD = perr = np.sqrt(np.diag(pcov))\n",
    "#             print(f'perr (SD of A, SD of k) {perr}')\n",
    "#             plt.clf()\n",
    "            if plot_in_console:\n",
    "                plt.scatter(x,y)\n",
    "                upper_x = 120\n",
    "                upper_y = max(y)+np.median(y)\n",
    "                if A*1.2 > upper_y > A:\n",
    "                    text_level = A\n",
    "                elif A*1.2 < upper_y:\n",
    "                    text_level = A*1.2\n",
    "                else:\n",
    "                    text_level = upper_y\n",
    "                plt.ylim(0,upper_y)\n",
    "                upper_t = math.ceil(max(x))\n",
    "                if upper_t > upper_x:\n",
    "                    upper_x = upper_t\n",
    "                curve_range = np.linspace(0,upper_x,upper_x*10)\n",
    "                plt.plot(curve_range, model(curve_range,A,k), color=color)\n",
    "                plt.annotate((f'A={A.round(3)}±{A_SD.round(3)}\\nk={k.round(3)}±{k_SD.round(3)}'), xy=(upper_x-40,text_level), xytext=(upper_x-40, text_level))\n",
    "    #             plt.show()\n",
    "\n",
    "        except:\n",
    "            coeffs = False\n",
    "#         print(f'coeffs (A, k) {coeffs}')\n",
    "    return coeffs, perr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_all_features_in_console():\n",
    "    this_data = df_sample_feature_nmol13C_permg\n",
    "    this_data.sort_values(by=['Total labelling time (min) corrected'])\n",
    "    use_data = pd.DataFrame()\n",
    "    for feature in this_data.iloc[:,1:]:\n",
    "#         plt.clf()\n",
    "        use_data['y'] = this_data[feature]\n",
    "        use_data['x'] = this_data['Total labelling time (min) corrected']\n",
    "        coeffs = curve_fitting(use_data, random_iterations)\n",
    "        print(type(coeffs))\n",
    "        if type(coeffs) == bool:\n",
    "            if coeffs == False:\n",
    "                chisquared = False\n",
    "        else:\n",
    "            chisquared = calculate_residuals(use_data,coeffs)\n",
    "        print(feature, coeffs, chisquared)\n",
    "print(\"defined functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e210ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_curve(x_list, y_list):\n",
    "    df_feature = pd.DataFrame({'x': x_list, 'y': y_list})\n",
    "    x = x_list\n",
    "    y = y_list\n",
    "    \n",
    "    '''\n",
    "    Generate Initial Curve\n",
    "    '''\n",
    "#     print(\"initial curve_fitting\")\n",
    "    coeffs = curve_fitting(df_feature,random_iterations=3, color='red', plot_in_console=True)\n",
    "\n",
    "#     '''\n",
    "#     Remove Outliers\n",
    "#     '''\n",
    "#     if type(coeffs) != bool:\n",
    "#         A, k = coeffs\n",
    "#         print(\"[A,k]:\",A,\",\",k)\n",
    "#         C = 0\n",
    "        \n",
    "#         try:\n",
    "#             H = (np.log((((A/2)/A)-1)*-1))/-k # half-life\n",
    "#             print(\"Half-life:\", H)\n",
    "#         except:\n",
    "#             H = np.nan\n",
    "#             print(\"Half-life:\", H)\n",
    "#         try:\n",
    "#             chisquared = calculate_residuals(df_feature, coeffs)\n",
    "#             RSDR = get_RSDR(coeffs, df_feature)\n",
    "#             print(\"Chi squard:\", chisquared)\n",
    "#             print(\"RSDR:\", RSDR)\n",
    "#         except:\n",
    "#             print(\"error in calculation chisquared or RSDR (possible poor curve fitting)\")\n",
    "#             chisquared = np.nan\n",
    "#             RSDR = np.nan\n",
    "#         print(\"Checking for outliers\")\n",
    "#         df_feature, outliers_detected, n_used = remove_outliers(df_feature, coeffs)\n",
    "#         print(\"Outliers detected and removed:\", outliers_detected)\n",
    "#         while outliers_detected > 0:\n",
    "#             print(\"Recalculating curve\")\n",
    "#             coeffs = curve_fitting(df_feature)\n",
    "#             if type(coeffs) != bool:\n",
    "#                 A,k = coeffs\n",
    "#                 print(\"[A,k]:\",A,\",\",k)\n",
    "#                 C = 0\n",
    "#                 try:\n",
    "#                     H = (np.log((((A/2)/A)-1)*-1))/-k # half-life\n",
    "#                     print(\"Half-life:\", H)\n",
    "#                 except:\n",
    "#                     H = np.nan\n",
    "#                     print(\"Half-life:\", H)\n",
    "#                 try:\n",
    "#                     chisquared = calculate_residuals(df_feature, coeffs)\n",
    "#                     RSDR = get_RSDR(coeffs, df_feature)\n",
    "#                     print(\"Chi squard:\", chisquared)\n",
    "#                     print(\"RSDR:\", RSDR)\n",
    "#                 except:\n",
    "#                     print(\"error in calculation chisquared or RSDR (possible poor curve fitting)\")\n",
    "#                     chisquared = np.nan\n",
    "#                     RSDR = np.nan\n",
    "#                 print(\"Checking for outliers\")\n",
    "#                 df_feature, outliers_detected, n_used = remove_outliers(df_feature, coeffs)\n",
    "#                 print(\"Outliers detected and removed:\", outliers_detected)\n",
    "#             else:\n",
    "#                 chisquared, A, k, C, H = [np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "#                 n_used = 0\n",
    "#                 break\n",
    "#     else:\n",
    "#         chisquared, A, k, C, H = [np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "#         n_used = 0\n",
    "#     lower_x = 0\n",
    "#     upper_x = 140\n",
    "#     step_x = 0.01\n",
    "    try:\n",
    "        if type(coeffs_initial) != bool:\n",
    "\n",
    "            t = curve_range = np.arange(0,upper_x,0.01)\n",
    "            s = curve_data = model(curve_range,A,k)\n",
    "        else:\n",
    "            chisquared, RSDR, A, k, C, H = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "            n_used = 0\n",
    "            t = np.nan\n",
    "            s = np.nan\n",
    "\n",
    "    except:\n",
    "        t = np.nan\n",
    "        s = np.nan\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "'''\n",
    "Example data set\n",
    "'''\n",
    "x_list = [1,0,3,4.75,5.666666667,5.75,7.6,10.78333333,12.18333333,14.78333333,15.73333333,17.75,17.76666667,17.81666667,23.53333333,24.16666667,25.76666667,27.31666667,29.93333333,32.66666667,35.88333333,44.78333333,47.5,59.58333333,67.88333333,89.76666667,89.83333333,107.2,119.7166667]\n",
    "y_list = [0.8,0,0.011635376,0.01205788,0.0509542,0.019029791,0.017741336,0.057440817,0.040689357,0.091717184,0.07081078,0.081946418,0.044566058,0.039326504,0.086661801,0.094272077,0.082630873,0.056210607,0.051231094,0.083449088,0.095866196,0.08559528,0.085047297,0.116140626,0.137561575,0.137194544,0.132068843,0.165178776,0.15894556]\n",
    "example_curve(x_list, y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed30728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac2a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_residuals(dataframe_x_y,coeffs):\n",
    "    A, k = coeffs\n",
    "    this_data = dataframe_x_y[pd.isna(dataframe_x_y.y) == False]\n",
    "    this_data['Predicted'] = model(this_data.x, A, k)\n",
    "    this_data['|Residual|'] = abs(this_data['Predicted'] - this_data['y'])\n",
    "    this_data['|Residual|^2'] = this_data['|Residual|']**2\n",
    "    chisquared = float(sum(this_data['|Residual|^2']))\n",
    "    return chisquared\n",
    "\n",
    "def initial_curve_plot(x_list,y_list, color, plot_in_console):\n",
    "    upper_x = 120 #Determines the x-limit of the chart\n",
    "    df_feature = pd.DataFrame({'x': x_list, 'y': y_list})\n",
    "    x = x_list\n",
    "    y = y_list\n",
    "    '''\n",
    "    Generate Initial Curve\n",
    "    '''\n",
    "    coeffs, perr = curve_fitting(df_feature,random_iterations=1, color=color, plot_in_console=plot_in_console)\n",
    "    A,k=coeffs\n",
    "    t = curve_range = np.arange(0,upper_x,0.01)\n",
    "    s = curve_data = model(curve_range,A,k)\n",
    "    '''\n",
    "    Check ChiSquared for data against curve (RSDR makes more sense for non-linear)\n",
    "    '''\n",
    "    ChiSquared = calculate_residuals(df_feature,coeffs)    \n",
    "    return(coeffs,perr,t,s,ChiSquared)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06afa47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc1464-3e85-4431-99a9-9f2b8952531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_excel(data_type, treatment_dfs_dictionary):\n",
    "    book_name = data_type + '_output'\n",
    "    excel_file = book_name + '.xlsx'\n",
    "    with pd.ExcelWriter(excel_file, engine = \"xlsxwriter\") as writer:\n",
    "    #     writer = pd.ExcelWriter(excel_file, engine = \"xlsxwriter\") # Assign name of function to use for excel manipulation (excel data file is assigned here)\n",
    "        for treatment,data_df in treatment_dfs_dictionary.items():\n",
    "            x_axis_column = 'Total labelling time (min) corrected' #set up the labels like this so I can send to function later if desired. Must match column header from dataframe\n",
    "            x_axis_label = str(x_axis_column)\n",
    "            y_axis_column = data_type\n",
    "            y_axis_label = str(y_axis_column)\n",
    "            cell_letter = 'D'\n",
    "            cell_number = 1\n",
    "            data_df[x_axis_column] = data_df[x_axis_column].astype(float)\n",
    "            data=data_df\n",
    "            end_index = data_df.columns.get_loc('Group') #First column to exclude\n",
    "            exp_stats = pd.DataFrame(index = ['A','SD of A', 'k', 'SD of k', 'Chi^2']) #Dataframe will hold info on curves for sheet\n",
    "            first_t = True\n",
    "            for feature in data_df.iloc[:,1:end_index]:\n",
    "                '''\n",
    "                Get curve for data\n",
    "                '''\n",
    "                df_for_curve = data_df[data_df[feature]>0]\n",
    "                x = df_for_curve['Total labelling time (min) corrected'].tolist()\n",
    "                x.append(0)\n",
    "                y = df_for_curve[feature].tolist()\n",
    "                y.append(0)\n",
    "                \n",
    "                plot_in_console=False\n",
    "                if plot_in_console:\n",
    "                    plt.clf()\n",
    "                coeffs,perr,t,s,ChiSquared = initial_curve_plot(x_list=x,y_list=y,color='black', plot_in_console=plot_in_console)\n",
    "                A_SD, k_SD = perr\n",
    "                A,k = coeffs\n",
    "                if plot_in_console:\n",
    "                    print(f'\\n{treatment} {feature}')\n",
    "                    print(f'A = {A.round(5)} +- {A_SD.round(5)}\\nk = {k.round(5)} +- {k_SD.round(5)}\\nChi^2 = {round(ChiSquared,5)}')\n",
    "                    plt.show()\n",
    "                '''\n",
    "                Add information about curve into dataframe for sheet\n",
    "                '''    \n",
    "                exp_stats[feature] = A,A_SD,k,k_SD,ChiSquared\n",
    "                '''\n",
    "                Add data required to plot curve in excel into dataframe for sheet\n",
    "                '''\n",
    "                if first_t:\n",
    "                    exp_curves = pd.DataFrame()\n",
    "                    exp_curves[x_axis_label] = t\n",
    "                    first_t = False\n",
    "                exp_curves[feature]=s\n",
    "            '''\n",
    "            Convert dataframe to excel format in selected sheet\n",
    "            '''\n",
    "            data_sheet_name = data_type + '-' + treatment\n",
    "            data.to_excel(writer, sheet_name = data_sheet_name)\n",
    "            exp_curves_sheet_name = 'Exp curves' + treatment\n",
    "            exp_curves.to_excel(writer, sheet_name = exp_curves_sheet_name)\n",
    "            exp_stats_sheet_name = 'Exp stats' + treatment\n",
    "            exp_stats.to_excel(writer, sheet_name = exp_stats_sheet_name)\n",
    "            '''\n",
    "            Start making excel charts (reiterate the treatment dataframe for clarity)\n",
    "            '''\n",
    "            for feature in data_df.iloc[:,1:end_index]:\n",
    "                '''\n",
    "                Data points as scatter plot first\n",
    "                '''\n",
    "                workbook = writer.book # workbook is the whole excel workbook\n",
    "                worksheet = writer.sheets[data_sheet_name] # worksheet is the individual sheet in the workbook        \n",
    "                max_rows = len(data) #data is the data_df in excel format\n",
    "                chart_title = feature\n",
    "                chart_cell = cell_letter + str(cell_number)    \n",
    "                col_x = data.columns.get_loc(x_axis_label) + 1 # Excel starts counting columns at 1 (not 0)\n",
    "                col_y = data.columns.get_loc(feature) + 1\n",
    "                this_chart = workbook.add_chart({'type': 'scatter'})\n",
    "                this_chart.add_series({\n",
    "                    'name': chart_title,\n",
    "                    'categories': [data_sheet_name, 1, col_x, max_rows, col_x],\n",
    "                    'values': [data_sheet_name, 1, col_y, max_rows, col_y],\n",
    "                    'marker': {'type': 'circle', 'size': 4},\n",
    "            #         'trendline': {'type': 'linear', 'display_equation': True, 'display_r_squared': True},\n",
    "                    'line': {'none': True},\n",
    "                    'border': {'none': True} \n",
    "                    })\n",
    "                '''\n",
    "                Exponential curve next\n",
    "                '''\n",
    "                if data_df[feature].count() > 3: # Don't bother plotting if there are not > 3 data points for this feature\n",
    "                    try:\n",
    "                        worksheet2 = writer.sheets[exp_curves_sheet_name] # worksheet is the individual sheet in the workbook\n",
    "                        max_rows_exponential = len(exp_curves)\n",
    "                        col_x_exponential = exp_curves.columns.get_loc(x_axis_label) + 1\n",
    "                        col_y_exponential = exp_curves.columns.get_loc(feature) + 1              \n",
    "                        this_chart.add_series(\n",
    "                            {\n",
    "                            'subtype': 'smooth',\n",
    "                            'name': 'exponential curve',\n",
    "                            'categories': [exp_curves_sheet_name, 1, col_x_exponential, max_rows_exponential, col_x_exponential],\n",
    "                            'values': [exp_curves_sheet_name, 1, col_y_exponential, max_rows_exponential, col_y_exponential],\n",
    "                            'marker': {'type': 'none'},\n",
    "                            'line': {'type': 'smooth', 'color': 'red'}\n",
    "                            })\n",
    "                        line_chart = workbook.add_chart({'type': 'scatter', 'subtype': 'smooth'})\n",
    "                        line_chart.add_series(\n",
    "                            {\n",
    "                            'name': chart_title,\n",
    "                            'categories': [sheet_name_exponential, 1, col_x_exponential, max_rows_exponential, col_x_exponential],\n",
    "                            'values': [sheet_name_exponential, 1, col_y_exponential, max_rows_exponential, col_y_exponential],\n",
    "                            'marker': {'type': 'none'},\n",
    "                            'line': {'color': 'red'}\n",
    "                    #         'border': {'none': True} \n",
    "                            })\n",
    "                    #     this_chart.add_series({'type': 'line'})\n",
    "                        curve_for_feature = True\n",
    "                    except:\n",
    "                        curve_for_feature = False\n",
    "                else:\n",
    "                    curve_for_feature = False\n",
    "                '''\n",
    "                Chart formatting\n",
    "                '''\n",
    "                this_chart.set_x_axis({\n",
    "                    'name': x_axis_label,\n",
    "                    'num_font': {'size': 12, 'name': 'Arial', 'color': 'black'},\n",
    "                    'name_font': {'size': 12, 'name': 'Arial', 'color': 'black', 'bold': False},\n",
    "                    'line': {'color': 'black'},\n",
    "                    'min': 0,\n",
    "                    'max': 130,\n",
    "                    'major_unit': 20,\n",
    "                    'crossing': 0,\n",
    "                    'major_gridlines': {'visible': False},\n",
    "                    'minor_gridlines': {'visible': False},\n",
    "                    'major_tick_marks': 'outside',\n",
    "                    'minor_tick_marks': 'none'\n",
    "                })\n",
    "                this_chart.set_y_axis({\n",
    "                    'name': y_axis_label,\n",
    "                    'num_font': {'size': 12, 'name': 'Arial', 'color': 'black'},\n",
    "                    'name_font': {'size': 12, 'name': 'Arial', 'color': 'black', 'bold': False},\n",
    "                    'line': {'color': 'black'},\n",
    "                    'min': 0,\n",
    "                    'crossing': 0,\n",
    "                    'major_gridlines': {'visible': False},\n",
    "                    'minor_gridlines': {'visible': False},\n",
    "                    'major_tick_marks': 'outside',\n",
    "                    'minor_tick_marks': 'none'\n",
    "                })\n",
    "\n",
    "                this_chart.set_title({\n",
    "                        'name': chart_title,\n",
    "                        'name_font': {'size': 12, 'bold': False, 'name':'Arial', 'color':'black'}\n",
    "                    })\n",
    "                this_chart.set_size({\n",
    "                        'width': 400, 'height':300\n",
    "                    })\n",
    "                this_chart.set_legend({\n",
    "                        'none': True\n",
    "                    })\n",
    "                this_chart.set_chartarea({\n",
    "                    'border': {'none': True},\n",
    "                    'fill':   {'color': 'white'}\n",
    "                })\n",
    "            #     line_chart.combine(this_chart)\n",
    "                worksheet.insert_chart(chart_cell, this_chart)\n",
    "                if curve_for_feature == True:\n",
    "                    worksheet2.insert_chart(chart_cell, line_chart)\n",
    "\n",
    "\n",
    "                cell_number += 16\n",
    "            print('charts plotted for', treatment, data_type)\n",
    "#     writer.save()\n",
    "    return print('\\ncharts plotted for', data_type, ' and saved to', excel_file, '\\n')\n",
    "plot_to_excel(data_type='F', treatment_dfs_dictionary=treatment_dfs_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07a464-dfd2-487f-9a5e-5048bc472c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def39252-c961-4e4f-abbb-eaaed1f2fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_curve(feature, treatment_dfs_dictionary,plot_in_console):\n",
    "    plt.clf()\n",
    "    color_list=['red','blue','orange','green','purple','yellow','cyan']\n",
    "    colorid=0\n",
    "    for name,df in treatment_dfs_dictionary.items():\n",
    "        df = df[df[feature]>0]\n",
    "        x = df['Total labelling time (min) corrected'].tolist()\n",
    "        x.append(0)\n",
    "        y = df[feature].tolist()\n",
    "        y.append(0)\n",
    "        if colorid < len(color_list):\n",
    "            color = color_list[colorid]\n",
    "        else:\n",
    "            colorid = 0\n",
    "            color = color_list[colorid]\n",
    "        \n",
    "        if plot_in_console:\n",
    "            print(f'{name} {feature} ({color})')\n",
    "            plt.plot(x, y, 'o', color=color, label = name)\n",
    "        else:\n",
    "            print(f'\\n{name} {feature}')\n",
    "        initial_curve_plot(x_list=x,y_list=y,color=color, plot_in_console=plot_in_console)\n",
    "        colorid +=1\n",
    "    if plot_in_console:\n",
    "        plt.title(label=feature)\n",
    "        plt.legend(loc = \"upper right\")\n",
    "        plt.xlabel('Total labelling time (min) corrected')\n",
    "        plt.ylabel('Normalized fractional labeling')\n",
    "        plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9c4cd-7b9a-4177-ad35-c7ec355cf978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8dfad7-5360-4a12-9540-7b359e21d9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in profile_df['Peak']:  \n",
    "    plot_label_curve(feature, treatment_dfs_dictionary, plot_in_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed429f46-5bb7-4755-9983-8e2299107228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6392dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_label_boxplot():\n",
    "    return\n",
    "\n",
    "def plot_area_boxplot(feature, treatment_dfs_dictionary_Areas, groupnames, yaxis):\n",
    "    boxplot_list = []\n",
    "    for name,df in treatment_dfs_dictionary_Areas.items():\n",
    "        x = df['Total labelling time (min) corrected'].tolist()\n",
    "        y = df[feature].tolist()\n",
    "        y_filtered = [value for value in y if np.isnan(value) == False]\n",
    "        boxplot_list.append(y_filtered)\n",
    "    #         plt.clf()\n",
    "    #         plt.scatter(x,y)\n",
    "    #         plt.title(label=feature)\n",
    "    #         plt.show()\n",
    "#             plt.plot(x, y, 'o')\n",
    "    try:\n",
    "        a,b = boxplot_list[0],boxplot_list[1]\n",
    "    #     t, p = ttest_ind(a, b, equal_var=False) # t-test\n",
    "        U1, p = MannwhitneyuResult = sps.mannwhitneyu(a,b)\n",
    "        SDa = np.std(a)\n",
    "        SDb = np.std(b)\n",
    "\n",
    "        try:\n",
    "            meana = np.mean(a)\n",
    "            meanb = np.mean(b)\n",
    "            plt.annotate(f'mean={meana.round(5)}\\nSD={SDa.round(5)}', xy=(1.1,meana), xytext=(1.1, meana))\n",
    "            plt.annotate(f'mean={meanb.round(5)}\\nSD={SDb.round(5)}\\np={p.round(5)}', xy=(2.1, meanb), xytext=(2.1, meanb))\n",
    "        except:\n",
    "            print(\"error\")\n",
    "        plt.boxplot(boxplot_list)\n",
    "        plt.title(label=feature)\n",
    "    #         plt.legend(treatments)\n",
    "        plt.xlabel('Group')\n",
    "\n",
    "        plt.xticks(np.arange(1,boxplot_ticks,1), labels=treatments, rotation=20)\n",
    "        plt.ylabel(yaxis)\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(f'Could not generate boxplot for {feature} {yaxis}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84576282-cea3-469d-b832-b82df05fe2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecb780",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not functional\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# def draw_vector(v0, v1, ax=None):\n",
    "#     ax = ax or plt.gca()\n",
    "#     arrowprops=dict(arrowstyle='->',\n",
    "#                     linewidth=2,\n",
    "#                     shrinkA=0, shrinkB=0)\n",
    "#     ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def pca_areas(df, features):\n",
    "#     df = df.replace(np.NaN, 0)\n",
    "# #     print(df[features])\n",
    "#     pca = PCA()\n",
    "#     components = pca.fit_transform(df[features])\n",
    "#     print(components)\n",
    "#     labels = {\n",
    "#         str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "#         for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "#     }\n",
    "#     print(labels)\n",
    "#     pca.fit(df[features])\n",
    "#     print(pca.components_)\n",
    "#     print(pca.explained_variance_)\n",
    "    \n",
    "#     # plot data\n",
    "#     plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "#     for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "#         v = vector * 3 * np.sqrt(length)\n",
    "#         draw_vector(pca.mean_, pca.mean_ + v)\n",
    "#     plt.axis('equal');\n",
    "\n",
    "\n",
    "# pca_areas(df=dictionary_of_dfs['rel_areas_df'], features=profile_df['Peak'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(0, 10, 30)\n",
    "# y = np.sin(x)\n",
    "# plt.plot(x, y, 'o', color='black');\n",
    "boxplot_ticks = number_of_treatments + 1\n",
    "legendlabels = np.repeat(treatments, 2)\n",
    "\n",
    "for feature in profile_df['Peak']:  \n",
    "    plot_label_curve(feature, treatment_dfs_dictionary, plot_in_console=True)\n",
    "    if use_areas == True:\n",
    "        plot_area_boxplot(feature, treatment_dfs_dictionary_RawAreas, boxplot_ticks, 'RawArea')\n",
    "#         plot_area_boxplot(feature, treatment_dfs_dictionary_RelAreas, boxplot_ticks, 'RelArea') # This is commented out because I suspect an error in RelArea table\n",
    "        \n",
    "if use_areas == True:\n",
    "    feature_list = profile_df['Peak'].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54754fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in profile_df['Peak']:  \n",
    "#     plot_label_curve(feature, treatment_dfs_dictionary, legendlabels)\n",
    "    for name,df in treatment_dfs_dictionary.items():\n",
    "        df = df[df[feature]>0]\n",
    "        x = df['Total labelling time (min) corrected'].tolist()\n",
    "        x.append(0)\n",
    "        y = df[feature].tolist()\n",
    "        y.append(0)\n",
    "\n",
    "        print(f'{name} ({feature})')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bb034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Retention index\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Adduct ratios\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compare to predicted F unlabeled (natural abundance)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Curves\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ac65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Graph into excel\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "set this up into modules based on function\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make super simple version for metaboanalyst upload.\n",
    "\n",
    "Add in curves, sample data etc...\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec702e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8082ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
